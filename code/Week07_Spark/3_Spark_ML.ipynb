{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark ML: Predicting Avocado Prices\n",
    "\n",
    "This notebook introduces how to train a ML model using Spark ML.  This bases on an excellent article in Towards Data Science [First Steps in Machine Learning with Apache Spark](https://towardsdatascience.com/first-steps-in-machine-learning-with-apache-spark-672fe31799a3) using [Avocado Prices dataset](https://www.kaggle.com/datasets/neuromusic/avocado-prices) in Kaggle.\n",
    "\n",
    "The objective of this model is to predict the average price of avocado given datetime, supply amounts, and region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Cluster Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    !apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "    !wget -q https://archive.apache.org/dist/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3.tgz\n",
    "    !tar xf spark-3.3.2-bin-hadoop3.tgz\n",
    "    !mv spark-3.3.2-bin-hadoop3 spark\n",
    "    !pip install -q findspark\n",
    "    import os\n",
    "    os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "    os.environ[\"SPARK_HOME\"] = \"/content/spark\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_url = 'local'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "        .master(spark_url)\\\n",
    "        .appName('Spark SQL')\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "First, we read a csv file.  We can provide option such as delimiter and header.  We then rename the colume names to remove dot ('.') in the names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'avocado.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_avocado = spark.read.csv(path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [c.replace(' ', '_') for c in df_avocado.columns]\n",
    "df_avocado = df_avocado.toDF(*cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_avocado.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then split data into training and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_avocado_train, df_avocado_test) = df_avocado.randomSplit([0.75, 0.25], seed=214)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create ML Pipeline\n",
    "For this pipeline, we will create several transformers using built-in estimators/transformers.  These include:\n",
    "\n",
    "\n",
    "| SparkML Feature | Feature Type | Data Type |\n",
    "|:-----------------|:--------------:|:--------------:|\n",
    "| SQLTransformer  | Tranformer   | Numerical |\n",
    "| MinMaxScaler    | Estimator    | Numerical |\n",
    "| StandardScaler  | Estimator    | Numerical |\n",
    "| StringIndexer   | Estimator    | Categorical |\n",
    "| VectorAssembler | Transformer  | Both |\n",
    "\n",
    "Using these components, we create the following pipeline:\n",
    "\n",
    "| Pipeline Stage | SparkML Feature |\n",
    "|:----------|:----------|\n",
    "| sql_transformer | SQLTransformer |\n",
    "| month_vec_asm_transfromer | VectorAssembler |\n",
    "| month_scaler_transfromer | MinMaxScaler |\n",
    "| numerical_vec_asm_transformer | VectorAssembler |\n",
    "| std_scaler_transformer | StandardScaler |\n",
    "| str_indexer_transformer | StringIndexer |\n",
    "| categorical_vec_asm_transformer | VectorAssembler |\n",
    "| all_vec_asm_transformer | VectorAssembler |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import SQLTransformer, MinMaxScaler, StandardScaler\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical Feature Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sql_transformer: numeric column selection and log-transform\n",
    "Create a transformer to select columns and log-transform some numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['AveragePrice', 'type']\n",
    "cols = [f\"`{col}`\" for col in cols]\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log_cols =  ['4225', '4770', 'Small_Bags', 'Large_Bags', 'XLarge_Bags']\n",
    "log_cols = [f\"LOG(`{col}`+1) AS `LOG_{col}`\" for col in log_cols]\n",
    "log_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statement = f\"\"\"SELECT{', '.join(cols)}, {', '.join(log_cols)}, \n",
    "    YEAR(__THIS__.Date)-2000 AS year, MONTH(__THIS__.Date) AS month\n",
    "    FROM __THIS__\n",
    "    \"\"\"\n",
    "statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_transformer = SQLTransformer(statement=statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avocado_train.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_transformer.transform(df_avocado_train).show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### month_vec_asm_transformer / month_scaler_transformer: create month vectors and normalize their values\n",
    "\n",
    "After using SQLTransformer, we then tranform *'month'* column into month vector and then normalize their values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "month_vec_asm_transformer = VectorAssembler(inputCols=['month'], outputCol='month_vec')\n",
    "\n",
    "df_avocado_month_ass = month_vec_asm_transformer.transform(sql_transformer.transform(df_avocado_train))\n",
    "df_avocado_month_ass.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a transformer that normalizes month vector using an estimator, *\"MinMaxScaler\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "month_scaler_estimator = MinMaxScaler(inputCol='month_vec', outputCol='month_scaled')\n",
    "month_scaler_transformer = month_scaler_estimator.fit(df_avocado_month_ass)\n",
    "\n",
    "month_scaler_transformer.transform(df_avocado_month_ass)\\\n",
    "    .select( ['month', 'month_vec', 'month_scaled'] )\\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### numerical_vec_asm_transformer/std_scaler_transformer : assemble numerical features vector and scale all numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_vec_asm_transformer = VectorAssembler(\n",
    "    inputCols=[\n",
    "      'year', 'month_scaled', 'LOG_4225', \n",
    "      'LOG_4770', 'LOG_Small_Bags', \n",
    "      'LOG_Large_Bags', 'LOG_XLarge_Bags'\n",
    "    ],\n",
    "    outputCol='features_num'\n",
    ")\n",
    "df_avocado_numerical = numerical_vec_asm_transformer.transform(month_scaler_transformer.transform(df_avocado_month_ass))\n",
    "df_avocado_numerical.select('year', 'month_scaled', 'LOG_4225','features_num').show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the numerical features using a StandardScaler\n",
    "std_scaler_estimator = StandardScaler(\n",
    "    inputCol=\"features_num\",\n",
    "    outputCol=\"features_scaled\",\n",
    "    withStd=True,\n",
    "    withMean=True\n",
    ")\n",
    "\n",
    "std_scaler_transformer = std_scaler_estimator.fit(df_avocado_numerical)\n",
    "std_scaler_transformer.transform(df_avocado_numerical).select(['features_scaled']).show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Feature Transformers\n",
    "Transforming categorical features usually involve text transformation e.g. one-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### str_indexer_transformer: encoding categorical data\n",
    "We create a transformer using \"StringIndexer\", which is an estimator that produces StringIndexerModel.  This is similar to perform one-hot encoder on the categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_indexer_estimator = StringIndexer(inputCol=\"type\", outputCol=\"type_index\")\n",
    "type_indexer_transformer = type_indexer_estimator.fit(df_avocado_train)\n",
    "\n",
    "type_indexer_transformer.transform(df_avocado_train)\\\n",
    "  .select( [\"type\", \"type_index\"] ).show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_vec_asm_transformer = VectorAssembler(\n",
    "    inputCols=['type_index'],\n",
    "    outputCol='features_cat'\n",
    ")\n",
    "categorical_vec_asm_transformer.transform(\n",
    "    type_indexer_transformer.transform(df_avocado_train)\n",
    ").select('type', 'type_index', 'features_cat').show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a pipeline: merge both numerical and categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vec_asm_transformer = VectorAssembler(\n",
    "        inputCols=['features_scaled', 'features_cat'],\n",
    "        outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_prep_pipeline = Pipeline(stages=[sql_transformer, month_vec_asm_transformer,\n",
    "                                         month_scaler_transformer, \n",
    "                                         numerical_vec_asm_transformer,\n",
    "                                         std_scaler_transformer,\n",
    "                                         type_indexer_transformer,\n",
    "                                         categorical_vec_asm_transformer,\n",
    "                                         all_vec_asm_transformer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_model = feature_prep_pipeline.fit(df_avocado_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform training dataset using the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avocado_train_transformed = pipeline_model.transform(df_avocado_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_avocado_train_transformed.select('features', 'AveragePrice').show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "We will train a linear regression model using transformed training dataset.  In order to do this, we will have to fit an estimator, *'LinearRegression'* to transformed training dataset to create a model, which is a *transformer*, that can be used to test the testing dataset.\n",
    "\n",
    "Note that this example focuses on how to create a pipeline.  Spark also provides hyperparameter tuning function.  However, this is out of the scope of this example.  Please refer to [First Steps in Machine Learning with Apache Spark](https://towardsdatascience.com/first-steps-in-machine-learning-with-apache-spark-672fe31799a3) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_reg_estimator = LinearRegression(\n",
    "    featuresCol='features',\n",
    "    labelCol='AveragePrice',\n",
    "    predictionCol='prediction',\n",
    "\n",
    "    # Hyperaparameters\n",
    "    maxIter=1000,\n",
    "    regParam=0.3,       # Regularization\n",
    "    elasticNetParam=0.8 # Regularization mixing parameter. 1 for L1, 0 for L2.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "linear_reg_model = linear_reg_estimator.fit(df_avocado_train_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference the testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avocado_train_pred = linear_reg_model.transform(df_avocado_train_transformed)\n",
    "df_avocado_train_pred.select(\n",
    "  ['AveragePrice', 'prediction']\n",
    ").sample(False, 0.1, 0).show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "Spark provides several evaluation functions.  We will have to select the right one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_eval = RegressionEvaluator(\n",
    "    labelCol='AveragePrice',\n",
    "    predictionCol='prediction',\n",
    "    metricName='rmse' # Root mean squared error\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_eval.evaluate(df_avocado_train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## THE END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
